{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "400131027_HW08.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install -U 'tensorflow-text==2.8.*'"
      ],
      "metadata": {
        "id": "b6YPEx1TVy-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gDodBJHeofED"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True , split=['train[:10%]', 'test[:10%]', 'validation[:10%]'])\n",
        "train , test, valid = data[0], data[1] , data[2]"
      ],
      "metadata": {
        "id": "BNPdmewxo5y6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
        "tf.keras.utils.get_file(\n",
        "    f'{model_name}.zip',\n",
        "    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
        "    cache_dir='.', cache_subdir='', extract=True\n",
        ")"
      ],
      "metadata": {
        "id": "TeMO8JsSK2a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34016b40-3594-42df-b830-51ed39b4da6f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./ted_hrlr_translate_pt_en_converter.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizers = tf.saved_model.load(model_name)"
      ],
      "metadata": {
        "id": "UVdT-fWUK9Ef"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 128\n",
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "eN9uqZrXhx6B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_max_tokens(pt, en):\n",
        "  num_tokens = tf.maximum(tf.shape(pt)[1],tf.shape(en)[1])\n",
        "  return num_tokens < MAX_TOKENS"
      ],
      "metadata": {
        "id": "7pPpmiwrhx3Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_pairs(pt, en):\n",
        "    pt = tokenizers.pt.tokenize(pt)\n",
        "    pt = pt.to_tensor()\n",
        "    en = tokenizers.en.tokenize(en)\n",
        "    en = en.to_tensor()\n",
        "    return pt, en"
      ],
      "metadata": {
        "id": "oCs7Kyjwhx0q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .filter(filter_max_tokens)\n",
        "      .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "\n",
        "train_batches = make_batches(train)\n",
        "val_batches = make_batches(valid)"
      ],
      "metadata": {
        "id": "DB_qvTFQhxyD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional Encoding"
      ],
      "metadata": {
        "id": "ibKWoD3w64Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = (1 / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :]//2)) / np.float32(d_model)) * np.arange(position)[:, np.newaxis])\n",
        "  \n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "cv_1E_XY68Fy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_encoding = positional_encoding(50, 10)\n",
        "print (pos_encoding.shape)\n",
        "\n",
        "plt.pcolormesh(pos_encoding[0])\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, 10))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "qmXdSHy77vP6",
        "outputId": "f15f1ee5-16a6-428f-9fd8-fe2f0346d105"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 50, 10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-gK7v5kB-qrl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masking"
      ],
      "metadata": {
        "id": "rcyc1vBZVH0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "metadata": {
        "id": "bX892LK9-qod"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "metadata": {
        "id": "oKqLtOCB-qgW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "metadata": {
        "id": "JtD6S8LoaLGy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-head attention"
      ],
      "metadata": {
        "id": "i4cRmlHGadgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)\n",
        "    k = self.wk(k) \n",
        "    v = self.wv(v)  \n",
        "\n",
        "    q = self.split_heads(q, batch_size) \n",
        "    k = self.split_heads(k, batch_size) \n",
        "    v = self.split_heads(v, batch_size) \n",
        "\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model)) \n",
        "\n",
        "    output = self.dense(concat_attention) \n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "ncFVyuYCaK2B"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Point wise feed forward network"
      ],
      "metadata": {
        "id": "z-5lLJOrakrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model)\n",
        "  ])"
      ],
      "metadata": {
        "id": "bF9GoZbaaKvC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder and decoder"
      ],
      "metadata": {
        "id": "B0W1x15HaqTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  \n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output) \n",
        "\n",
        "    ffn_output = self.ffn(out1)  \n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  \n",
        "\n",
        "    return out2"
      ],
      "metadata": {
        "id": "NSD07ry1aKoF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask) \n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  \n",
        "\n",
        "    ffn_output = self.ffn(out2)  \n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2) \n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "id": "URso8r8daKgy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(MAX_TOKENS, self.d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  "
      ],
      "metadata": {
        "id": "xpHxoOu1aKZa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(MAX_TOKENS, d_model)\n",
        "\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x) \n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    return x, attention_weights"
      ],
      "metadata": {
        "id": "JVAh2n6RaKSm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "U_P3wdseaKMK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# transformer model"
      ],
      "metadata": {
        "id": "d_TzroioZcRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self,*, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           input_vocab_size=input_vocab_size, rate=rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           target_vocab_size=target_vocab_size, rate=rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    inp, tar = inputs\n",
        "\n",
        "    padding_mask, look_ahead_mask = self.create_masks(inp, tar)\n",
        "\n",
        "    enc_output = self.encoder(inp, training, padding_mask) \n",
        "\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "    final_output = self.final_layer(dec_output) \n",
        "\n",
        "    return final_output, attention_weights\n",
        "\n",
        "  def create_masks(self, inp, tar):\n",
        "    padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return padding_mask, look_ahead_mask"
      ],
      "metadata": {
        "id": "sSkra5JfaKJC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ],
      "metadata": {
        "id": "u7ZjhA_0aKCy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "3u72wwh5aJ_3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "metadata": {
        "id": "gvnk3bzEaJ8i"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss and metrics"
      ],
      "metadata": {
        "id": "oje9Aj9rbG1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "metadata": {
        "id": "dAgSCGN9aJ1y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "SunEpQ9kaJyq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "valid_accuracy = tf.keras.metrics.Mean(name='valid_accuracy')"
      ],
      "metadata": {
        "id": "Cm-Xa95ZaJvi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and checkpointing"
      ],
      "metadata": {
        "id": "YFjkqZDibNdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=8000,\n",
        "    target_vocab_size=8000,\n",
        "    rate=dropout_rate)"
      ],
      "metadata": {
        "id": "PXMO9FGKaJs1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "lzBI5puqaJnD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64)\n",
        "]\n",
        "\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer([inp, tar_inp],\n",
        "                                 training = True)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))\n",
        "\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def valid_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer([inp, tar_inp],\n",
        "                                 training = False)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "  \n",
        "    valid_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "metadata": {
        "id": "PnqZbx_waJkC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  valid_accuracy.reset_states()\n",
        "\n",
        "  # inp -> portuguese, tar -> english\n",
        "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "    train_step(inp, tar)\n",
        "  \n",
        "  for (batch, (inp, tar)) in enumerate(val_batches):\n",
        "    valid_step(inp, tar)\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}  Validation Accuracy {valid_accuracy.result():.4f}')"
      ],
      "metadata": {
        "id": "4uTr8qHTbp4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (batch, (inp, tar)) in enumerate(val_batches):\n",
        "    print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWgxtHgg7uUq",
        "outputId": "92d52f6f-b411-4cde-f9f0-2894c90b88dd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, tokenizers, transformer):\n",
        "    self.tokenizers = tokenizers\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "    # input sentence is portuguese, hence adding the start and end token\n",
        "    assert isinstance(sentence, tf.Tensor)\n",
        "    if len(sentence.shape) == 0:\n",
        "      sentence = sentence[tf.newaxis]\n",
        "\n",
        "    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
        "\n",
        "    encoder_input = sentence\n",
        "\n",
        "    # As the output language is english, initialize the output with the\n",
        "    # english start token.\n",
        "    start_end = self.tokenizers.en.tokenize([''])[0]\n",
        "    start = start_end[0][tf.newaxis]\n",
        "    end = start_end[1][tf.newaxis]\n",
        "\n",
        "    # `tf.TensorArray` is required here (instead of a python list) so that the\n",
        "    # dynamic-loop can be traced by `tf.function`.\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    for i in tf.range(max_length):\n",
        "      output = tf.transpose(output_array.stack())\n",
        "      predictions, _ = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "      # select the last token from the seq_len dimension\n",
        "      predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "      # concatentate the predicted_id to the output which is given to the decoder\n",
        "      # as its input.\n",
        "      output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    output = tf.transpose(output_array.stack())\n",
        "    # output.shape (1, tokens)\n",
        "    text = tokenizers.en.detokenize(output)[0]  # shape: ()\n",
        "\n",
        "    tokens = tokenizers.en.lookup(output)[0]\n",
        "\n",
        "    # `tf.function` prevents us from using the attention_weights that were\n",
        "    # calculated on the last iteration of the loop. So recalculate them outside\n",
        "    # the loop.\n",
        "    _, attention_weights = self.transformer([encoder_input, output[:,:-1]], training=False)\n",
        "\n",
        "    return text, tokens, attention_weights"
      ],
      "metadata": {
        "id": "HY5dgXypbp1Z"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(tokenizers, transformer)"
      ],
      "metadata": {
        "id": "Tm-ZtRVvbpyx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ],
      "metadata": {
        "id": "ygupR5osbpwV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for pt, en in test.take(20):\n",
        "  i = i + 1\n",
        "  print(\"#\" , i ,\"text is : \\n\")\n",
        "  sentence = pt.numpy().decode('utf-8')\n",
        "  ground_truth = en.numpy().decode('utf-8')\n",
        "\n",
        "  translated_text, translated_tokens, attention_weights = translator(\n",
        "      tf.constant(sentence))\n",
        "  print_translation(sentence, translated_text, ground_truth)\n",
        "\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWDnKfI7bpti",
        "outputId": "fab776b7-47e8-4ea1-d955-846a0163f32f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 1 text is : \n",
            "\n",
            "Input:         : depois , podem fazer-se e testar-se previsões .\n",
            "Prediction     : and you know , you ' re doing it .\n",
            "Ground truth   : then , predictions can be made and tested .\n",
            "\n",
            "\n",
            "# 2 text is : \n",
            "\n",
            "Input:         : forçou a parar múltiplos laboratórios que ofereciam testes brca .\n",
            "Prediction     : it ' s not only to take the best way to be able to be .\n",
            "Ground truth   : it had forced multiple labs that were offering brca testing to stop .\n",
            "\n",
            "\n",
            "# 3 text is : \n",
            "\n",
            "Input:         : as formigas são um exemplo clássico ; as operárias trabalham para as rainhas e vice-versa .\n",
            "Prediction     : the same is auticreproitation , they ' re all sorts of their own body .\n",
            "Ground truth   : ants are a classic example ; workers work for queens and queens work for workers .\n",
            "\n",
            "\n",
            "# 4 text is : \n",
            "\n",
            "Input:         : uma em cada cem crianças no mundo nascem com uma doença cardíaca .\n",
            "Prediction     : a typrolentrial in a car .\n",
            "Ground truth   : one of every hundred children born worldwide has some kind of heart disease .\n",
            "\n",
            "\n",
            "# 5 text is : \n",
            "\n",
            "Input:         : neste momento da sua vida , ela está a sofrer de sida no seu expoente máximo e tinha pneumonia .\n",
            "Prediction     : in this day , i ' m going to get my mother and then she ' d been able to get her to get hertment .\n",
            "Ground truth   : at this point in her life , she 's suffering with full-blown aids and had pneumonia .\n",
            "\n",
            "\n",
            "# 6 text is : \n",
            "\n",
            "Input:         : onde estão as redes económicas ?\n",
            "Prediction     : where are there ?\n",
            "Ground truth   : where are economic networks ?\n",
            "\n",
            "\n",
            "# 7 text is : \n",
            "\n",
            "Input:         : ( aplausos )\n",
            "Prediction     : ( applause )\n",
            "Ground truth   : ( applause )\n",
            "\n",
            "\n",
            "# 8 text is : \n",
            "\n",
            "Input:         : eu usei os contentores de transporte , e também os alunos ajudaram-nos a fazer toda a mobília dos edifícios , para torná-los confortáveis​​ , dentro do orçamento do governo , mas também com a mesma a área da casa , mas muito mais confortável .\n",
            "Prediction     : i ' ve been been been back to the same way , and they ' re going to get a lot of other people in the world , but in the world , and then they ' re all over to get them in the world .\n",
            "Ground truth   : i used the shipping container and also the students helped us to make all the building furniture to make them comfortable , within the budget of the government but also the area of the house is exactly the same , but much more comfortable .\n",
            "\n",
            "\n",
            "# 9 text is : \n",
            "\n",
            "Input:         : e , no entanto , a ironia é que a única maneira de podermos fazer qualquer coisa nova é dar um passo nessa direção .\n",
            "Prediction     : and , if you ' re a little bit of a little bit of it ' s like to be able to get a new to get a way .\n",
            "Ground truth   : and yet , the irony is , the only way we can ever do anything new is to step into that space .\n",
            "\n",
            "\n",
            "# 10 text is : \n",
            "\n",
            "Input:         : a luz nunca desaparece .\n",
            "Prediction     : it ' s never been being .\n",
            "Ground truth   : the light never goes out .\n",
            "\n",
            "\n",
            "# 11 text is : \n",
            "\n",
            "Input:         : `` agora , `` '' tweets '' '' , quem está a `` '' tweetar '' '' ? ''\n",
            "Prediction     : ` ` ` ` ` ' ' ' ' ' ' ' ' ' ' one people in the question ? ' '\n",
            "Ground truth   : now , tweets , who 's tweeting ?\n",
            "\n",
            "\n",
            "# 12 text is : \n",
            "\n",
            "Input:         : no egito , por exemplo , 91 % das mulheres que vivem hoje no egito foram mutiladas sexualmente dessa forma .\n",
            "Prediction     : in the last year , we had been used to get the same time to get the same time to the same time .\n",
            "Ground truth   : in egypt , for instance , 91 percent of all the females that live in egypt today have been sexually mutilated in that way .\n",
            "\n",
            "\n",
            "# 13 text is : \n",
            "\n",
            "Input:         : por outro lado , os bebés de 15 meses ficavam a olhar para ela durante muito tempo caso ela agisse como se preferisse os brócolos , como se não percebessem a situação .\n",
            "Prediction     : in the last year , i ' ve got a very long time to get a n ' t just been able to get her , 000 years , 000 dollars with her days .\n",
            "Ground truth   : on the other hand , 15 month-olds would stare at her for a long time if she acted as if she liked the broccoli , like they could n't figure this out .\n",
            "\n",
            "\n",
            "# 14 text is : \n",
            "\n",
            "Input:         : naquele momento , percebi quanta energia negativa é precisa para conservar aquele ódio dentro de nós .\n",
            "Prediction     : in this case , it ' s a swollentl , it ' s not a lot of them .\n",
            "Ground truth   : in that instant , i realized how much negative energy it takes to hold that hatred inside of you .\n",
            "\n",
            "\n",
            "# 15 text is : \n",
            "\n",
            "Input:         : e a discussão é : o que é que isso interessa .\n",
            "Prediction     : and then you know , what is the question .\n",
            "Ground truth   : and the discussion is , who cares ? right ?\n",
            "\n",
            "\n",
            "# 16 text is : \n",
            "\n",
            "Input:         : se escolhermos um lugar e formos cuidadosos , as coisas estarão sempre lá quando as procurarmos .\n",
            "Prediction     : if you can see , you ' re not a little bit of them , you ' re not .\n",
            "Ground truth   : if you designate a spot and you 're scrupulous about it , your things will always be there when you look for them .\n",
            "\n",
            "\n",
            "# 17 text is : \n",
            "\n",
            "Input:         : é um museu muito popular agora , e criei um grande monumento para o governo .\n",
            "Prediction     : and you can see a little bit of a little bit of the whole way .\n",
            "Ground truth   : it 's a very popular museum now , and i created a big monument for the government .\n",
            "\n",
            "\n",
            "# 18 text is : \n",
            "\n",
            "Input:         : é completamente irrelevante .\n",
            "Prediction     : it ' s always like . it ' s always like .\n",
            "Ground truth   : it 's completely irrelevant .\n",
            "\n",
            "\n",
            "# 19 text is : \n",
            "\n",
            "Input:         : todos defenderam que a sua técnica era a melhor , mas nenhum deles tinha a certeza disso e admitiram-no .\n",
            "Prediction     : he was all of them , and he had been able to be able to be the middle of the middle of the middle of them , and he was there .\n",
            "Ground truth   : `` they all argued that , `` '' my technique is the best , '' '' but none of them actually knew , and they admitted that . ''\n",
            "\n",
            "\n",
            "# 20 text is : \n",
            "\n",
            "Input:         : a partir daquele momento , comecei a pensar .\n",
            "Prediction     : so , i ' m going to think about this idea .\n",
            "Ground truth   : at that moment , i started thinking .\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention_head(in_tokens, translated_tokens, attention):\n",
        "  # The plot is of the attention when a token was generated.\n",
        "  # The model didn't generate `<START>` in the output. Skip it.\n",
        "  translated_tokens = translated_tokens[1:]\n",
        "\n",
        "  ax = plt.gca()\n",
        "  ax.matshow(attention)\n",
        "  ax.set_xticks(range(len(in_tokens)))\n",
        "  ax.set_yticks(range(len(translated_tokens)))\n",
        "\n",
        "  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n",
        "  ax.set_xticklabels(\n",
        "      labels, rotation=90)\n",
        "\n",
        "  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n",
        "  ax.set_yticklabels(labels)"
      ],
      "metadata": {
        "id": "XO9wMwMpotQL"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention_weights(sentence, translated_tokens, attention_heads):\n",
        "  in_tokens = tf.convert_to_tensor([sentence])\n",
        "  in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n",
        "  in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n",
        "  in_tokens\n",
        "\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "  for h, head in enumerate(attention_heads):\n",
        "    ax = fig.add_subplot(2, 4, h+1)\n",
        "\n",
        "    plot_attention_head(in_tokens, translated_tokens, head)\n",
        "\n",
        "    ax.set_xlabel(f'Head {h+1}')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "CbmyjqCHovqL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Eu li sobre triceratops na enciclopédia.'\n",
        "Ground_truth = \"I read about triceratops in the encyclopedia.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, Ground_truth)\n",
        "\n",
        "plot_attention_weights(sentence, translated_tokens, attention_weights['decoder_layer4_block2'][0])"
      ],
      "metadata": {
        "id": "tet980f4t0a4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}